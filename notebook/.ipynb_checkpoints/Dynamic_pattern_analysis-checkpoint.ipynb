{
 "metadata": {
  "name": "",
  "signature": "sha256:eba1768bf4a2360bd2199911ccbe8e8d82968bc13966bcd3ff666b4dbe71d221"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Dynamic brain pattern analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Researchers:\n",
      "\n",
      "- Kirell Benzi\n",
      "- Benjamin Ricaud\n",
      "\n",
      "This notebook aims at extracting and classfying activation patterns of the brain from a spatio-temporal graph of active brain components."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Reference Yeo ids\n",
      "\n",
      "| Functional id | Name |\n",
      "|---------------|------|\n",
      "| 1 | Visual |\n",
      "| 2 | Somatomotor |\n",
      "| 3\t| Dorsal attention |\n",
      "| 4\t| Ventral Attention |\n",
      "| 5\t| Limbic |\n",
      "| 6\t| Frontoparietal |\n",
      "| 7\t| Default |"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import os\n",
      "import math\n",
      "import csv\n",
      "import networkx as nx\n",
      "import scipy as sp\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io\n",
      "import json\n",
      "from collections import Counter, OrderedDict, defaultdict, Callable\n",
      "import sklearn as sk\n",
      "import sklearn.cluster\n",
      "import itertools\n",
      "\n",
      "%matplotlib inline\n",
      "# Current notebook folder, change the path here\n",
      "NOTEBOOK_DIR = '/Users/kikohs/work/pro/dybrain/notebook/'\n",
      "NOTEBOOK_INPUT_DIR = os.path.join(NOTEBOOK_DIR, 'data/')\n",
      "NOTEBOOK_OUT_DIR = os.path.join(NOTEBOOK_DIR, 'data/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Load the spatio-temporal graph and create a global networkx graph"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "List the different version of the dynamic components graphm"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def list_graph_files():\n",
      "    for root, dirs, files in os.walk(NOTEBOOK_INPUT_DIR):\n",
      "        return [filename for filename in files if filename.startswith('dynamic_components')]\n",
      "\n",
      "graphs = list_graph_files()\n",
      "print(graphs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "basename = 'dynamic_components'\n",
      "\n",
      "path = os.path.join(NOTEBOOK_INPUT_DIR, graphs[0])\n",
      "\n",
      "blob = json.load(open(path, 'r'))\n",
      "G = nx.Graph()  # it is a directed graph\n",
      "\n",
      "# Create nodes\n",
      "for n in blob['nodes']:\n",
      "    nid = int(n['id'])\n",
      "    n.pop('id', None)\n",
      "    G.add_node(nid, n)\n",
      "\n",
      "# Create edges\n",
      "for e in blob['edges']:\n",
      "    src = int(e['source'])\n",
      "    tgt = int(e['target'])\n",
      "    eid = int(e['id'])\n",
      "    G.add_edge(src, tgt, {'eid': eid})\n",
      "\n",
      "# Add extra graph properties\n",
      "G.graph['layer_count'] = blob['layer_count']\n",
      "G.graph['component_count'] = blob['component_count']\n",
      "G.graph['group_count'] = blob['group_count']\n",
      "\n",
      "print(nx.info(G))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Split the global graph into components\n",
      "\n",
      "The graph is a sum of disconnected components. We want to analyze those components to extract similar patterns. Let start by creating some helper functions and a class which represent a component (it is a subgraph of the global graph)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bitsets import bitset  # pip install bitsets\n",
      "\n",
      "# Read labels and\n",
      "BRAIN_LABELS = []\n",
      "filename = os.path.join(NOTEBOOK_INPUT_DIR,'input/brain_labels_68.txt')\n",
      "with open(filename, 'r') as f:\n",
      "    reader = csv.reader(f, delimiter=',')\n",
      "    for row in reader:\n",
      "        BRAIN_LABELS.append(row[0])\n",
      "print('Read: ' + filename)\n",
      "\n",
      "FUNC_IDS = []\n",
      "filename = os.path.join(NOTEBOOK_INPUT_DIR,'input/brain_functional_ids.txt')\n",
      "with open(filename, 'r') as f:\n",
      "    reader = csv.reader(f, delimiter=',')\n",
      "    for row in reader:\n",
      "        FUNC_IDS.append(row[0])\n",
      "print('Read: ' + filename)\n",
      "\n",
      "filename = os.path.join(NOTEBOOK_INPUT_DIR, 'BrainGraph68.txt')\n",
      "A = np.loadtxt(open(filename,\"rb\"), dtype=int, delimiter=',', skiprows=0)\n",
      "# Remove diagonal elements to have a real adjacency matrix\n",
      "A = A - np.diag(np.diag(A))\n",
      "AG = nx.from_numpy_matrix(A)\n",
      "print('Read: ' + filename), '\\n'\n",
      "print 'Average ajacency matrix:', A.shape\n",
      "\n",
      "FUNC_ZONES = 7\n",
      "FUNC_NODE_COUT = A.shape[0]\n",
      "X_SPACING = 3\n",
      "Y_SPACING = 3\n",
      "\n",
      "def create_node_data(layer_id, input_id, weight,\n",
      "                brain_lab=BRAIN_LABELS, brain_func_ids=FUNC_IDS):\n",
      "    data = {}\n",
      "    data['ts_group_id'] = 0\n",
      "    data['layer_pos'] = layer_id\n",
      "    data['input_id'] = input_id\n",
      "    data['func_id'] = brain_func_ids[input_id - 1]  # 0 to 67 entries\n",
      "    data['weight'] = weight\n",
      "    data['x'] = layer_id * X_SPACING\n",
      "    data['y'] = input_id * Y_SPACING\n",
      "    data['pos'] = (data['x'], data['y'])\n",
      "\n",
      "    name = brain_lab[input_id - 1]\n",
      "    short_name = ''\n",
      "    brain_side = ''\n",
      "    if name.startswith('lh_'):\n",
      "        short_name = name[3:]\n",
      "        brain_side = 'left'\n",
      "    elif name.startswith('rh_'):\n",
      "        short_name = name[3:]\n",
      "        brain_side = 'right'\n",
      "    else:\n",
      "        short_name = name\n",
      "    data['label'] = short_name\n",
      "    data['brain_side'] = brain_side\n",
      "    data['name'] = name\n",
      "\n",
      "    return data\n",
      "\n",
      "def build_graph_from_activated_layers(feature, tol, input_graph=AG):\n",
      "    X = feature.reshape((-1, input_graph.number_of_nodes()))\n",
      "    G = nx.Graph()\n",
      "    nb_layers = X.shape[0]\n",
      "    # Create the domain (input_ids) range\n",
      "    ids = tuple(np.arange(1, X.shape[1] + 1))\n",
      "    input_ids = bitset('Graph', ids)\n",
      "    \n",
      "    # Create edges and nodes if needed\n",
      "    for i in xrange(nb_layers-1):\n",
      "        current_nodes = input_ids.frombools(X[i] > tol)\n",
      "        next_nodes = input_ids.frombools(X[i+1] > tol)\n",
      "        for c in current_nodes.members():\n",
      "            src = c + (i * len(ids))\n",
      "            for n in next_nodes.members():\n",
      "                tgt = n + ((i+1) * len(ids))\n",
      "                # Check that input_id are real adajcent \n",
      "                # in the global adjacency matrix\n",
      "                if input_graph.has_edge(c, n) or c == n:\n",
      "                    \n",
      "                    if src not in G:\n",
      "                        data = create_node_data(i, c, X[i, c-1])\n",
      "                        G.add_node(src, data)\n",
      "                    \n",
      "                    if tgt not in G:\n",
      "                        data = create_node_data(i+1, n, X[i+1, n-1])\n",
      "                        G.add_node(tgt, data)\n",
      "                    \n",
      "                    G.add_edge(src, tgt)\n",
      "    return G\n",
      "\n",
      "\n",
      "def build_graph_from_feature_tuples(X, tol, input_graph=AG):\n",
      "    G = nx.Graph()\n",
      "    # the list is ordered by layer and input_id\n",
      "    nb_layers = X[-1][0][0] + 1\n",
      "    input_node_count = input_graph.number_of_nodes()\n",
      "    \n",
      "    # Create edges, and crate nodes if needed\n",
      "    for i in xrange(nb_layers-1):\n",
      "        # for each current node\n",
      "        for c in itertools.ifilter(lambda x: x[0][0] == i and x[1] > tol, X):\n",
      "            input_id = c[0][1] + 1\n",
      "            src = input_id + (i * input_node_count)\n",
      "            for n in itertools.ifilter(lambda x: x[0][0] == i+1 and x[1] > tol, X):\n",
      "                tgt_in_id = n[0][1] + 1\n",
      "                tgt = tgt_in_id + ((i+1) * input_node_count)\n",
      "                # Check that input_id are real adajcent in the global adjacency matrix\n",
      "                if input_graph.has_edge(input_id, tgt_in_id) or input_id == tgt_in_id:\n",
      "                    \n",
      "                    if src not in G:\n",
      "                        data = create_node_data(i, input_id, c[1])\n",
      "                        G.add_node(src, data)\n",
      "                    \n",
      "                    if tgt not in G:\n",
      "                        data = create_node_data(i+1, tgt_in_id, n[1])\n",
      "                        G.add_node(tgt, data)\n",
      "                    \n",
      "                    G.add_edge(src, tgt)\n",
      "    \n",
      "    return G"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "class Component(object):\n",
      "    def __init__(self, component_id):\n",
      "        self.component_id = component_id\n",
      "        self.g = nx.Graph()\n",
      "        self.layers = None\n",
      "        self.input_ids = None\n",
      "        self.feat = None\n",
      "        self.ts_group = 0\n",
      "        self.cluster_id = None\n",
      "        self.func_ids = None\n",
      "        self.compfeat = None  # compressed features\n",
      "        self.subgraphs = None\n",
      "    \n",
      "    def add_node(self, nid, data):\n",
      "        self.g.add_node(nid, data)\n",
      "    \n",
      "    def add_edge(self, src, tgt, data):\n",
      "        self.g.add_edge(src, tgt, data)\n",
      "    \n",
      "    def reconstruct_from_array(self, feature, tol=0.0):\n",
      "        self.g = build_graph_from_activated_layers(feature, tol)\n",
      "        self.extract_properties()\n",
      "    \n",
      "    def reconstruct_from_list(self, ordered_feat, tol=0.0):\n",
      "        self.g = build_graph_from_feature_tuples(ordered_feat, tol)\n",
      "        self.extract_properties()\n",
      "        \n",
      "    def extract_properties(self):\n",
      "        self.layers = []\n",
      "        self.feat = []\n",
      "        self.func_ids = []\n",
      "        \n",
      "        inIds = Counter()\n",
      "        layer_positions = OrderedDict()\n",
      "        last_pos = 0\n",
      "        \n",
      "        # iter through nodes to extract properties such as\n",
      "        # layers, width, height, features, compressed features\n",
      "        for n, d in self.g.nodes_iter(data=True):\n",
      "            self.ts_group = d['ts_group_id']\n",
      "            lp = d['layer_pos']\n",
      "            input_id = d['input_id']\n",
      "            self.func_ids.append(int(d['func_id']))\n",
      "            \n",
      "            # needed to compute relative postion (0, 1, 2) of this component\n",
      "            if lp not in layer_positions:\n",
      "                layer_positions[lp] = last_pos\n",
      "                last_pos += 1\n",
      "            \n",
      "            relative_pos = layer_positions[lp]\n",
      "            self.feat.append((relative_pos, input_id))\n",
      "            inIds.update( { input_id : 1} )\n",
      "            \n",
      "        self.layers = sorted(layer_positions.keys())\n",
      "        self.input_ids = OrderedDict(sorted(inIds.iteritems(), key=lambda t: t[0]))\n",
      "        # Compute probability for each input id\n",
      "        self.compfeat = [(k, float(v)/self.width()) for k, v in self.input_ids.iteritems()]\n",
      "        # Extract subgraphs\n",
      "        self.subgraphs = nx.connected_component_subgraphs(self.g)\n",
      "        \n",
      "    def width(self):\n",
      "        return len(self.layers)\n",
      "    \n",
      "    def height(self):\n",
      "        \"\"\"\n",
      "        Return the count of input_ids (original brain node ids) \n",
      "        over which the component spans\n",
      "        \"\"\"\n",
      "        return len(self.input_ids)\n",
      "    \n",
      "    def features(self):\n",
      "        \"\"\"\n",
      "        Return list of pairs where tuple[0] is the layer \n",
      "        number and tuple[1] the position where the node is activated\n",
      "        \"\"\"\n",
      "        return self.feat\n",
      "    \n",
      "    def compressed_features(self):\n",
      "        \"\"\"\n",
      "        Return a list of pair where pair[0] is a tuple (layer, input_id)\n",
      "        and pair[1] the weight assocatied to this entry\n",
      "        \"\"\"\n",
      "        return self.compfeat\n",
      "    \n",
      "    def node_count(self):\n",
      "        return self.g.number_of_nodes()\n",
      "    \n",
      "    def edge_count(self):\n",
      "        return self.g.number_of_edges()\n",
      "    \n",
      "    def layers(self):\n",
      "        return self.layers\n",
      "    \n",
      "    def __str__(self):\n",
      "        s = 'Component id: ' + str(self.component_id) + '\\n'\n",
      "        if self.cluster_id:\n",
      "            s += ' cluster id: ' + str(self.cluster_id) + '\\n'\n",
      "        if len(self.subgraphs) > 0:\n",
      "            s += ' subgraphs: ' + str(len(self.subgraphs)) + '\\n'\n",
      "        r = ' group: ' + str(self.ts_group) + '\\n' \\\n",
      "            ' #nodes: ' + str(self.g.number_of_nodes()) + '\\n' \\\n",
      "            ' #edges: ' + str(self.g.number_of_edges()) + '\\n' \\\n",
      "            ' #width: ' + str(self.width()) + ' -- ' + ', '.join(map(str, self.layers)) + '\\n' \\\n",
      "            ' #height ' + str(self.height()) + '\\n'\n",
      "        return s + r\n",
      "    \n",
      "    def __repr__(self):\n",
      "        return self.__str__()\n",
      "    \n",
      "    def _draw_graph(self, fig, ax, g):\n",
      "        pos = nx.get_node_attributes(g, 'pos')\n",
      "        color = nx.get_node_attributes(g, 'weight').values()\n",
      "        nx.draw_networkx_edges(g, pos, alpha=0.4)\n",
      "        nx.draw_networkx_nodes(g, pos,\n",
      "                               node_size=80,\n",
      "                               node_color=color,\n",
      "                               cmap=plt.cm.Reds_r,\n",
      "                               vmin=0.0, vmax=1.0)\n",
      "\n",
      "        label_start_x = -7\n",
      "        label_end_x = -2\n",
      "        for k, p in pos.iteritems():\n",
      "            lab = g.node[k]['name'] + ' ' + str(g.node[k]['input_id'])\n",
      "            ax.text(label_start_x, p[1], lab, fontsize=7)\n",
      "\n",
      "        last_layer = self.layers[-1]\n",
      "        full_width = last_layer + 1\n",
      "        x_labels = np.arange(full_width)\n",
      "        x_ticks = np.arange(0, (full_width * X_SPACING), X_SPACING)\n",
      "        \n",
      "        plt.xticks(x_ticks, x_labels, size='large')\n",
      "        plt.xlim(label_end_x+1, label_end_x + X_SPACING + last_layer * X_SPACING)\n",
      "        \n",
      "        ax.set_yticklabels([])\n",
      "        ax.grid(False)\n",
      "    \n",
      "    def draw(self, figid=None, figsize=None):\n",
      "        if self.g.number_of_nodes() == 0:\n",
      "            print 'Empty graph for component', self.component_id\n",
      "            return\n",
      "        subgraph_height = 3\n",
      "        if not figsize:\n",
      "            figsize = (12, len(self.subgraphs) * subgraph_height + 1)\n",
      "        fig = plt.figure(figid, figsize=figsize)\n",
      "        fig.set_visible(False)\n",
      "        fig.suptitle('Patterns of component ' + str(self.component_id), fontsize=14, fontweight='bold')\n",
      "        ax1 = None\n",
      "        for i, s in enumerate(self.subgraphs):\n",
      "            ax = fig.add_subplot(len(self.subgraphs), 1, i)\n",
      "            if i == 0:\n",
      "                ax1 = ax\n",
      "            self._draw_graph(fig, ax, s)\n",
      "        ax1.set_xlabel('time step')  # set only for last figure\n",
      "        return fig"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TMP to remove\n",
      "# c = rebuild_component_from_cluster(0, grouped_components[7], RECONSTRUCT_TOL)\n",
      "# f = c.draw()\n",
      "# f.set_visible(True)\n",
      "# f.savefig(os.path.join(NOTEBOOK_OUT_DIR, 'test.png'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we can represent a component, let's extract them from the global graph"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create dictionnary of Components\n",
      "components = []\n",
      "for c in xrange(G.graph['component_count']):\n",
      "    components.append(Component(c))\n",
      "\n",
      "# Split nodes in each component\n",
      "input_ids = set()\n",
      "for n, d in G.nodes_iter(data=True):\n",
      "    comp_id = d['component_num']\n",
      "    components[comp_id].add_node(n, d)\n",
      "    input_ids.add(d['input_id']) \n",
      "    \n",
      "# Split edges in each component\n",
      "for u, v, d in G.edges_iter(data=True):\n",
      "    src = G.node[u]\n",
      "    tgt = G.node[v]\n",
      "    \n",
      "    # edges cannot exist between 2 different components\n",
      "    if src['component_num'] != tgt['component_num']:\n",
      "        raise Exception('Edge between 2 diff components', 'exp')\n",
      "    \n",
      "    comp_id = src['component_num']\n",
      "    # Create a DIRECTED edge from layer position (time)\n",
      "    if src['layer_pos'] > tgt['layer_pos']:\n",
      "        components[comp_id].add_edge(v, u, d)\n",
      "    elif src['layer_pos'] < tgt['layer_pos']: \n",
      "        components[comp_id].add_edge(u, v, d)\n",
      "    else:\n",
      "        raise Exception('same layer', 'exp')\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since components belong to different patients, it would be useful to group them by patients.\n",
      "\n",
      "Let's do a Patient class"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Patient:\n",
      "    def __init__(self, patient_id):\n",
      "        self.pid = patient_id\n",
      "        self.components = []\n",
      "    \n",
      "    def add_component(self, comp):\n",
      "        self.components.append(comp)\n",
      "    \n",
      "    def aggregate(self, comp_prop):\n",
      "        return map(lambda x: getattr(x, comp_prop)(), self.components)\n",
      "    \n",
      "    def apply_on_components(self, func, comp_prop):\n",
      "        # Dynamically call funtion defined by user on each component method\n",
      "        return func(self.aggregate(comp_prop))\n",
      "        \n",
      "    def mean(self, comp_prop):\n",
      "        if not self.components:\n",
      "            return None    \n",
      "        f = lambda x: float(sum(x))/ len(x)\n",
      "        # calc mean\n",
      "        m = self.apply_on_components(f, comp_prop)\n",
      "        # standard dev\n",
      "        std = math.sqrt(f(map(lambda x: (x-m)**2, self.aggregate(comp_prop))))\n",
      "        return m, std\n",
      "    \n",
      "    def component_count(self):\n",
      "        return len(self.components)\n",
      "    \n",
      "    def node_count(self):\n",
      "        return self.apply_on_components(sum, 'node_count')\n",
      "    \n",
      "    def edge_count(self):\n",
      "        return self.apply_on_components(sum, 'edge_count')\n",
      "        \n",
      "    def __str__(self):\n",
      "        s = 'Patient id: ' + str(self.pid) + '\\n' \\\n",
      "            '#components: ' + str(self.component_count()) + '\\n' \\\n",
      "            '#nodes: ' + str(self.node_count()) + '\\n' \\\n",
      "            '#edges: ' + str(self.edge_count()) + '\\n' \\\n",
      "            'width (mean, std): ' + str(self.mean('width')) + '\\n' \\\n",
      "            'height (mean, std): ' + str(self.mean('height')) + '\\n'\n",
      "        return s "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create patients\n",
      "patients = [Patient(p) for p in xrange(G.graph['group_count'])]\n",
      "\n",
      "# Extract useful properties from our components\n",
      "for v in components:\n",
      "    v.extract_properties()\n",
      "    patients[v.ts_group].add_component(v)  # store components in patients"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Show some insights about the newly extracted components"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Component repartition per group"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(1, (7,5))\n",
      "index = np.arange(len(patients))\n",
      "components_per_patient = [p.component_count() for p in patients]\n",
      "rects1 = plt.bar(index, components_per_patient)\n",
      "plt.xlabel('Patients')\n",
      "plt.ylabel('#components')\n",
      "plt.title('Components per patient')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Nodes and edges per patient"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index = np.arange(len(patients))\n",
      "nodes_per_patient = [p.node_count() for p in patients]\n",
      "edges_per_patient = [p.edge_count() for p in patients]\n",
      "\n",
      "fig = plt.figure(num=2, figsize=(8, 5), dpi=326, facecolor='w', edgecolor='k')\n",
      "ax1 = fig.add_subplot(1,2,1) # 1 row, 2 columns, first plot\n",
      "rects1 = ax1.bar(index, nodes_per_patient)\n",
      "plt.xlabel('Patients')\n",
      "plt.ylabel('#nodes')\n",
      "plt.title('Nodes per patient')\n",
      "\n",
      "ax2 = fig.add_subplot(1,2,2)\n",
      "rects1 = ax2.bar(index, edges_per_patient)\n",
      "plt.xlabel('Patients')\n",
      "plt.ylabel('#edges')\n",
      "plt.title('Edges per patient')\n",
      "\n",
      "fig.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Width and height histograms for components before filtering"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(1, (10, 6))\n",
      "ax1 = fig.add_subplot(1,2,1) # 1 row, 2 columns, first plot\n",
      "widths = [c.width() for c in components]\n",
      "unique_width = set(widths)\n",
      "ax1.hist(widths, len(unique_width), range=[0, 25])\n",
      "plt.xlabel('Width')\n",
      "plt.ylabel('#Component')\n",
      "plt.title('Component per width')\n",
      "# plt.xticks(np.arange(2, 25))  # Cannot have a width < 2 (1 edge is at least 2 timesteps)\n",
      "\n",
      "\n",
      "ax2 = fig.add_subplot(1,2,2)\n",
      "heights = [c.height() for c in components]\n",
      "unique_height = set(heights)\n",
      "ax2.hist(heights, len(unique_height), range=[1, 20])\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('#Component')\n",
      "plt.title('Component per height')\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Filter noisy components"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Set filtering params"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filt_min_width = 3\n",
      "filt_max_width = 8\n",
      "filt_min_height = 2\n",
      "filt_max_height = 11"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filtered_components = [c for c in components \n",
      "               if (filt_min_height <= c.height() <= filt_max_height) and (filt_min_width <= c.width() <= filt_max_width)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Before filtering:', len(components), 'components'\n",
      "print 'After filtering:', len(filtered_components), 'components'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Components after filtering"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "del patients\n",
      "# Create patients\n",
      "patients = [Patient(p) for p in xrange(G.graph['group_count'])]\n",
      "\n",
      "# Extract useful properties from our components\n",
      "for v in filtered_components:\n",
      "    v.extract_properties()\n",
      "    patients[v.ts_group].add_component(v)  # store components in patients"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(1, (7,5))\n",
      "index = np.arange(len(patients))\n",
      "components_per_patient = [p.component_count() for p in patients]\n",
      "rects1 = plt.bar(index, components_per_patient)\n",
      "plt.xlabel('Patients')\n",
      "plt.ylabel('#components')\n",
      "plt.title('Components per patient')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(1, (10, 8))\n",
      "ax1 = fig.add_subplot(1,2,1) # 1 row, 2 columns, first plot\n",
      "widths = [c.width() for c in filtered_components]\n",
      "ax1.hist(widths, 16, range=[0, 15])\n",
      "plt.xlabel('Width')\n",
      "plt.ylabel('#Component')\n",
      "plt.title('Component per width')\n",
      "# plt.xticks(np.arange(2, 25))  # Cannot have a width < 2 (1 edge is at least 2 timesteps)\n",
      "\n",
      "\n",
      "ax2 = fig.add_subplot(1,2,2)\n",
      "heights = [c.height() for c in filtered_components]\n",
      "ax2.hist(heights, 100, range=[0, filt_max_height])\n",
      "plt.xlabel('Height')\n",
      "plt.ylabel('#Component')\n",
      "plt.title('Component per height')\n",
      "# plt.xticks(np.arange(40))\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " # Pattern extraction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this part, we will try to extract repeating patterns across components using different methods."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a function to build a graph from an matrix of activated nodes or list of tuples"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Create a sparse feature matrix from our filtered components"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Each component will be vectorized so the number of columns in the matrix is number_of_nodes * actual_max_width\n",
      "actual_max_width = max(widths) + 1  # the bound is inclusive\n",
      "feat_columns = FUNC_NODE_COUT * actual_max_width\n",
      "feat_rows = len(filtered_components)\n",
      "# Create matrix of features\n",
      "A = sp.sparse.lil_matrix( (feat_rows, feat_columns), dtype=np.int8)\n",
      "print 'Feature matrix:', A.shape\n",
      "# Fill values\n",
      "for i, c in enumerate(filtered_components):  # for each component\n",
      "    for f in c.features():  # for each feature of a component\n",
      "        # calc offset, f[0] is the layer number, f[1] the input_id\n",
      "        x = f[0] * FUNC_NODE_COUT + f[1]\n",
      "        A[i, x] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The matrix A now contains our initial features, we will try to classify or learn a dictionnary of patterns based on those features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## K-means analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's run the k-means algorithm, and set the cluster_id in each component"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster_count = 16\n",
      "km = sk.cluster.KMeans(n_clusters=cluster_count, init='k-means++',\n",
      "                       n_init=30, max_iter=300, tol=0.0001,\n",
      "                       precompute_distances=True, verbose=0, random_state=None, copy_x=True, n_jobs=4)\n",
      "\n",
      "clusters = km.fit_predict(A)\n",
      "# Set cluster_id in the components\n",
      "hist_comp = defaultdict(int)\n",
      "for i, c in enumerate(clusters):\n",
      "    filtered_components[i].cluster_id = c\n",
      "    hist_comp[c] += 1\n",
      "\n",
      "sort_func = lambda x: x.cluster_id\n",
      "filtered_components = sorted(filtered_components, key=sort_func)\n",
      "grouped_components = [list(g) for k, g in itertools.groupby(filtered_components, sort_func)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's plot some insights about our clusters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(1, (7,5))\n",
      "bar_width = 1\n",
      "plt.bar(np.arange(cluster_count), hist_comp.values(), bar_width)\n",
      "\n",
      "plt.xlabel('Cluster count')\n",
      "plt.ylabel('#Component')\n",
      "plt.title('Component per cluster')\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For each cluster, we look at the repartition of components' nodes and group them by their functional ids"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(1, (16,8))\n",
      "nn = cluster_count\n",
      "\n",
      "for i in xrange(nn):\n",
      "    plt.subplot(2, nn/2, i+1)\n",
      "    active_nodes = list(itertools.chain(*[ c.func_ids for c in grouped_components[i]]))\n",
      "    plt.hist(active_nodes, FUNC_ZONES)\n",
      "    plt.title('Cluster id: ' + str(i))\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the repartition of nodes by clusters, we will extract similar patterns of nodes. A pattern is a subgraph of a component which repeats between across the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "RECONSTRUCT_TOL = 0.2  # reconstrcution threshold"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Rebuild average components from centroids"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "centroids = km.cluster_centers_\n",
      "comp_centroid = []\n",
      "for i, c in enumerate(centroids):\n",
      "    p = Component(i)\n",
      "    p.reconstruct_from_array(c, RECONSTRUCT_TOL)\n",
      "    p.cluster_id = i\n",
      "    comp_centroid.append(p)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Rebuild average components from clusters\n",
      "\n",
      "All the component features from the same cluster are added to form an histogram of appearance for each input_id per layer. The weight of each node is the probability of having the node activated at each time step in this cluster."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rebuild_component_from_cluster(comp_id, comp_list, tol):\n",
      "    \"\"\"Extract average component from a list of\n",
      "    component in the same cluster\n",
      "    \"\"\"\n",
      "    # Histogram\n",
      "    counter = defaultdict(int) \n",
      "    for c in comp_list:\n",
      "        for pair in c.features():\n",
      "            counter[pair] += 1\n",
      "    # order by key\n",
      "    ordered_feat = sorted(counter.iteritems())\n",
      "    # Count for each layer how many nodes are activated\n",
      "    pCount = Counter(map(lambda x: x[0][0], ordered_feat))\n",
      "    pCountS = sorted(pCount.iteritems())\n",
      "    \n",
      "    # Create probability for each node    \n",
      "    proba_feat = []\n",
      "    for i in ordered_feat:\n",
      "        cur_layer = i[0][0]\n",
      "        elem = (i[0], float(i[1]) / pCountS[cur_layer][1])\n",
      "        proba_feat.append(elem)\n",
      "    \n",
      "    p = Component(comp_id)\n",
      "    p.cluster_id = comp_list[0].cluster_id\n",
      "    p.reconstruct_from_list(proba_feat, tol)\n",
      "    \n",
      "    return p\n",
      "\n",
      "comp_cluster = []\n",
      "for i, c in enumerate(grouped_components): \n",
      "    comp_cluster.append(rebuild_component_from_cluster(i, c, RECONSTRUCT_TOL))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "avg_comp_cluster = comp_cluster[0]\n",
      "avg_comp_centroid = comp_centroid[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The centroid is actually an histogram of the components in the cluster, both method should yield to the same result"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print avg_comp_cluster\n",
      "print avg_comp_centroid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Depending on the threshold on the reconstruction, the average component can be composed of a sum of Patterns (which are in fact real patterns ..)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subgraphs = nx.connected_component_subgraphs(avg_comp_cluster.g)\n",
      "print len(subgraphs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Plot average components insights"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(1, (14,5))\n",
      "ax1 = fig.add_subplot(1,2,1) # 1 row, 2 columns, first plot\n",
      "ax1.bar(np.arange(len(comp_cluster)), [c.height() for c in comp_cluster], 1)\n",
      "plt.xlabel('AvComponent id')\n",
      "plt.ylabel('Height')\n",
      "plt.title('AvComponent heights')\n",
      "\n",
      "ax2 = fig.add_subplot(1,2,2) # 1 row, 2 columns, first plot\n",
      "ax2.bar(np.arange(len(comp_cluster)), [c.width() for c in comp_cluster], 1)\n",
      "plt.xlabel('AvComponent id')\n",
      "plt.ylabel('Width')\n",
      "plt.title('AvComponent widths')\n",
      "\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(1, (16,15))\n",
      "for i, p in enumerate(comp_cluster):\n",
      "    ax = fig.add_subplot(4, len(comp_cluster)/4, i+1)\n",
      "    # create histo for each pattern\n",
      "    counter = Counter(p.func_ids)\n",
      "    # add 0 for mimssing values\n",
      "    for f in xrange(1, FUNC_ZONES+1):\n",
      "        if f not in counter:\n",
      "            counter[f] = 0\n",
      "    # ordered by func id\n",
      "    val = OrderedDict(sorted(counter.iteritems(), key=lambda t: t[0]))\n",
      "    # plot\n",
      "    ax.bar(np.arange(1, FUNC_ZONES+1), val.values(), 0.8)\n",
      "    plt.xlabel('Functional id')\n",
      "    plt.ylabel('#nodes')\n",
      "    plt.title('Repartition of nodes for AvComponent ' + str(i))\n",
      "    plt.xticks(np.arange(1, FUNC_ZONES+1))\n",
      "    \n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Let's try k-means on compressed features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We create a new sparse matrix for our components. Each component is now represented by a sparse input vector of input_ids and the probability to have an activated node for each component"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filtered_components2 = filtered_components\n",
      "feat_columns = FUNC_NODE_COUT\n",
      "feat_rows = len(filtered_components)\n",
      "# Create matrix of features\n",
      "B = sp.sparse.lil_matrix( (feat_rows, feat_columns), dtype=np.float64)\n",
      "print 'Compressed features matrix:', B.shape\n",
      "# Fill values\n",
      "for i, c in enumerate(filtered_components2):  # for each component\n",
      "    for f in c.compressed_features():\n",
      "        idx = f[0] - 1\n",
      "        B[i, idx] = f[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster_count2 = 16\n",
      "km2 = sk.cluster.KMeans(n_clusters=cluster_count2, init='k-means++',\n",
      "                       n_init=30, max_iter=300, tol=0.0001,\n",
      "                       precompute_distances=True, verbose=0, random_state=None, copy_x=True, n_jobs=4)\n",
      "\n",
      "clusters2 = km2.fit_predict(B)\n",
      "# Set cluster_id in the components\n",
      "for i, c in enumerate(clusters2):\n",
      "    filtered_components2[i].cluster_id = c\n",
      "\n",
      "sort_func = lambda x: x.cluster_id\n",
      "filtered_components2 = sorted(filtered_components2, key=sort_func)\n",
      "grouped_components2 = [list(g) for k, g in itertools.groupby(filtered_components2, sort_func)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Plot insights on clusters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(1, (7,5))\n",
      "bar_width = 1\n",
      "\n",
      "plt.hist([k.cluster_id for k in filtered_components2], cluster_count2)\n",
      "\n",
      "plt.xlabel('Cluster2 count')\n",
      "plt.ylabel('#CComponent')\n",
      "plt.title('CComponent per cluster')\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For each cluster, we look at the repartition of components' nodes and group them by their functional ids"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "fig = plt.figure(1, (16,8))\n",
      "nn = cluster_count2\n",
      "\n",
      "for i in xrange(nn):\n",
      "    plt.subplot(2, nn/2, i+1)\n",
      "    active_nodes = list(itertools.chain(*[ c.func_ids for c in grouped_components2[i]]))\n",
      "    plt.hist(active_nodes, FUNC_ZONES)\n",
      "    plt.title('Cluster id: ' + str(i))\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Rebuild average components from clusters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "RECONSTRUCT_TOL2 = 0.09"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "comp_cluster2 = []\n",
      "for i, c in enumerate(grouped_components2): \n",
      "    comp_cluster2.append(rebuild_component_from_cluster(i, c, RECONSTRUCT_TOL2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "fig = plt.figure(1, (14,5))\n",
      "ax1 = fig.add_subplot(1,2,1) # 1 row, 2 columns, first plot\n",
      "ax1.bar(np.arange(len(comp_cluster2)), [c.height() for c in comp_cluster2], 1)\n",
      "plt.xlabel('AvCComponent id')\n",
      "plt.ylabel('Height')\n",
      "plt.title('AvCComponent heights')\n",
      "\n",
      "ax2 = fig.add_subplot(1,2,2) # 1 row, 2 columns, first plot\n",
      "ax2.bar(np.arange(len(comp_cluster2)), [c.width() for c in comp_cluster2], 1)\n",
      "plt.xlabel('AvCComponent id')\n",
      "plt.ylabel('Width')\n",
      "plt.title('AvCComponent widths')\n",
      "\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Let's draw some patterns"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for c in comp_cluster:\n",
      "    f = c.draw()\n",
      "    f.set_visible(True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for c in comp_cluster2:\n",
      "    f = c.draw(figsize=(12,12))\n",
      "    f.set_visible(True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Dictionary learning"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}